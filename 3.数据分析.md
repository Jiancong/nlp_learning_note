## 数据读取与分析

本章主要内容为数据读取和数据分析，具体使用Pandas库完成数据读取操作，并对赛题数据进行分析构成。
#### 学习目标：
 - 学习使用Pandas读取赛题数据
 - 分析赛题数据的分布规律

#### 数据读取
赛题数据虽然是文本数据，每个新闻是不定长的，但任然使用csv格式进行存储。因此可以直接用Pandas完成数据读取的操作。

    import pandas as pd
    train_df = pd.read_csv('/home/tianchi/myspace/user_data/train_set.csv', sep='\t', nrows=50000)
    
这里的`read_csv`由三部分构成：

- 读取的文件路径，这里需要根据改成你本地的路径，可以使用相对路径或绝对路径，在这里我使用了私有的空间数据，其实可以挂载别人放在天池公共数据集里的，这样可以不用下载；

- 分隔符`sep`，为每列分割的字符，设置为`\t`即可；
- 读取行数`nrows`，为此次读取文件的函数，是数值类型（由于数据集比较大，建议先设置为100，我在这里设置为了50000）；

然后用head看看数据集头部数据块

    train_df.head()

#### 数据分析

在读取完成数据集后，我们还可以对数据集进行数据分析的操作。虽然对于非结构数据并不需要做很多的数据分析，但通过数据分析还是可以找出一些规律的。

此步骤我们读取了所有的训练集数据，在此我们通过数据分析希望得出以下结论：

- 赛题数据中，新闻文本的长度是多少？
- 赛题数据的类别分布是怎么样的，哪些类别比较多？
- 赛题数据中，字符分布是怎么样的？

#### **句子长度分析**
使用下面的方法可以查看，注意lambda表达式的运用

    %pylab inline
    train_df['text_len'] = train_df['text'].apply(lambda x: len(x.split(' ')))
    print(train_df['text_len'].describe())
================================
Populating the interactive namespace from numpy and matplotlib
count    50000.000000
mean       904.589900
std        961.345267
min          2.000000
25%        373.000000
50%        670.500000
75%       1123.000000
max      44665.000000
Name: text_len, dtype: float64
=======================================

对新闻句子的统计可以得出，本次赛题给定的文本比较长，每个句子平均由904个字符构成，最短的句子长度为2，最长的句子长度为44665。真是够长的啊！！！

下图将句子长度绘制了直方图，可见大部分句子的长度都几种在2000以内。

    _ = plt.hist(train_df['text_len'], bins=100)
    plt.xlabel('Text char count')
    plt.title("Histogram of char count")

![句子字符长度的直方图](https://github.com/Jiancong/nlp_learning_note/blob/master/sentence_length_histo.png?raw=true)

#### 新闻类别分布

接下来可以对数据集的类别进行分布统计，具体统计每类新闻的样本个数。

    train_df['label'].value_counts().plot(kind='bar')
    plt.title('News class count')
    plt.xlabel("category")
    
![新闻类别分类统计](https://github.com/Jiancong/nlp_learning_note/blob/master/news_category_count.png?raw=true)

在数据集中标签的对应的关系如下：{'科技': 0, '股票': 1, '体育': 2, '娱乐': 3, '时政': 4, '社会': 5, '教育': 6, '财经': 7, '家居': 8, '游戏': 9, '房产': 10, '时尚': 11, '彩票': 12, '星座': 13}

从统计结果可以看出，赛题的数据集类别分布存在较为不均匀的情况。在训练集中科技类新闻最多，其次是股票类新闻，最少的新闻是星座新闻。`请注意这是在采样50000个样本下的结论。`

#### **字符分布统计**

接下来可以统计每个字符出现的次数，首先可以将训练集中所有的句子进行拼接进而划分为字符，并统计每个字符的个数。

从统计结果中可以看出，在训练集中总共包括6180个字，其中编号3750的字出现的次数最多，编号1766等的字出现的次数最少。

    from collections import Counter
    all_lines = ' '.join(list(train_df['text']))
    word_count = Counter(all_lines.split(" "))
    #print("word_count:\n", word_count)
    word_count = sorted(word_count.items(), key=lambda d:d[1], reverse = True)
    
    print(len(word_count))
    
    print(word_count[0])
    
    print(word_count[1])
    print(word_count[2])
    print(word_count[-3])
    print(word_count[-2])
    print(word_count[-1])
    
==============================
6180
('3750', 1863795)
('648', 1225648)
('900', 810253)
('5764', 1)
('5390', 1)
('1766', 1)
============================
这里还可以根据字在每个句子的出现情况，反推出标点符号。下面代码统计了不同字符在句子中出现的次数，其中字符3750，字符900和字符648在采样的5w新闻的覆盖率接近99%，很有可能是标点符号。

    from collections import Counter
    
    train_df['text_unique'] = train_df['text'].apply(lambda x: ' '.join(set(x.split(' '))))
    
    all_lines = ' '.join(list(train_df['text_unique']))
    #print(train_df['text_unique'][0])
    #print(train_df['text_unique'][1])
    #print(all_lines)
    word_count = Counter(all_lines.split(" "))
    word_count = sorted(word_count.items(), key=lambda d:int(d[1]), reverse = True)
    print(word_count[0])
    print(word_count[1])
    print(word_count[2])

=============================
('3750', 49504)
('900', 49433)
('648', 47998)

================================

### **数据分析的结论**

通过上述分析我们可以得出以下结论：

1. 赛题中每个新闻包含的字符个数平均为1000-2000个，还有一些新闻字符较长；
2. 赛题中新闻类别分布不均匀，科技类新闻样本量在采样的5w中接近1w，星座类新闻样本量不到100；
3. 赛题总共包括7000-8000个字符；

通过数据分析，我们还可以得出以下结论：

1. 每个新闻平均字符个数较多，可能需要截断；

2. 由于类别不均衡，会严重影响模型的精度；

### **本章小结**

本章对赛题数据进行读取，并新闻句子长度、类别和字符进行了可视化分析。

### **本章作业**

1. 假设字符3750，字符900和字符648是句子的标点符号，请分析赛题每篇新闻平均由多少个句子构成？

2. 统计每类新闻中出现次数对多的字符

### **作业答案**


    
    import re
    train_df['sentence_num'] = train_df['text'].apply(lambda x:len( re.split('3750|900|648', x)))
    print(train_df['sentence_num'].describe())


